{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object localization\n",
    "One last time, recall our two guiding questions for this week:\n",
    "\n",
    "- _What_ is in an image (e.g. debris, buildings, etc.)?\n",
    "- _Where_ are these things located _in 3D space_ ?\n",
    "\n",
    "Let's take stock of everything we've learned this week. The first two days, we learned about structure from motion. We saw how, so long as you have two images with some overlap, you can reconstruct a scene up to scale in an arbitrary reference frame. If you have some additional information (e.g. GPS location) and can afford to make some additional assumptions (e.g. the ground is flat), you can have a coarse georegistration of every image that is part of the structure from motion sequence. \n",
    "\n",
    "We spent the next two days discussing deep learning. We saw that machine learning can pick up patterns in the data that can aid in the general classification problem. We also saw that, using convolutional neural networks, we were able to extend this approach to the classification of images. \n",
    "\n",
    "Are we done?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xBD and Image Segmentation\n",
    "*Image segmentation* is a related but distinct problem from image classification. Rather than asking \"is there flooding in the image?\", it asks \"where is there flooding in an image?\". This is a problem that has seen interest in recent years, as it is no longer enough to just say whether there is a human in an image (think self-driving cars). One of the largest image segmentation datasets is called xBD. Here's what a sample pair of images looks like: \n",
    "\n",
    "<img src=\"notebook_images/hurricane-harvey_00000006_post_disaster.png\" width=\"250\"  />\n",
    "\n",
    "<img src=\"notebook_images/targets.png\" width=\"250\"  />\n",
    "\n",
    "Just as the output of our classification CNN was a single label, the output of a segmentation CNN is an entirely new image! \n",
    "In a sense, image segmentation is just pixel-wise classification. This means that image segmentation is a much more challenging task than just image classification. Accordingly, neural networks trained for image segmentation are much more complex than most classification CNN's. \n",
    "\n",
    "## Unet\n",
    "One of the more popular architectures for image segmentation is called unet. It was originally invented to do image segmentation for biomedical purposes. The structure of the unet is shown below:\n",
    "\n",
    "<img src=\"notebook_images/unet.png\" width=\"500\"  />\n",
    "\n",
    "Unet features a contracting path in the first half that provides discriminative feature extraction. The second half is an expanding path that does the actual localization of these features in the image. \n",
    "\n",
    "The dataset creation for image segmentation is much more complex than for image classification: absent any other data sources, someone has to literally sit down and click on polygons that contain the target region. For georeferenced satellite imagery this might not be too hard, since you could just overlay OpenStreetMap data on top of the image, but for CAP imagery like ours a single image could take upwards to half an hour. \n",
    "\n",
    "## Class Activation Maps\n",
    "The following approach is adapted from the following paper by Zhou et al: https://arxiv.org/pdf/1512.04150.pdf. \n",
    "\n",
    "Zhou et al found that neural networks that were trained for classification still retain a good amount of localization capability. Therefore, by doing some clever manipulations to our neural network, we can recover some of that localization capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "import requests\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "try:\n",
    "    import cv2\n",
    "except:\n",
    "    !pip install opencv-python\n",
    "    import cv2\n",
    "import pdb\n",
    "from cnn_finetune import make_model\n",
    "\n",
    "net = make_model('resnet18', num_classes=2, pretrained=False)\n",
    "finalconv_name = '_features'\n",
    "PATH = \"flood_checkpoint_epoch0.pth\"\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hook the feature extractor\n",
    "features_blobs = []\n",
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.data.cpu().numpy())\n",
    "\n",
    "net._modules.get(finalconv_name).register_forward_hook(hook_feature)\n",
    "\n",
    "# get the softmax weight\n",
    "params = list(net.parameters())\n",
    "weight_softmax = np.squeeze(params[-2].data.numpy())\n",
    "\n",
    "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "    # generate the class activation maps upsample to 256x256\n",
    "    size_upsample = (512, 512)\n",
    "    bz, nc, h, w = feature_conv.shape\n",
    "    output_cam = []\n",
    "    for idx in class_idx:\n",
    "        cam = weight_softmax[idx].dot(feature_conv.reshape((nc, h*w)))\n",
    "        cam = cam.reshape(h, w)\n",
    "        cam = cam - np.min(cam)\n",
    "        cam_img = cam / np.max(cam)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "    return output_cam\n",
    "\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "   mean=[0.485, 0.456, 0.406],\n",
    "   std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "preprocess = transforms.Compose([\n",
    "   transforms.Resize(768),\n",
    "   transforms.RandomCrop(512),\n",
    "   transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"test_images/010_1282_99809b0c-fc64-46e5-957f-7ff8e7547d8c.jpg\"\n",
    "img_pil = Image.open(img_path)\n",
    "\n",
    "img_tensor = preprocess(img_pil)\n",
    "img_array = cv2.cvtColor(np.round(255*img_tensor.permute(1, 2, 0).numpy()), cv2.COLOR_BGR2RGB)\n",
    "cv2.imwrite('test_image.jpg', img_array)\n",
    "\n",
    "img_tensor = normalize(img_tensor)\n",
    "img_variable = Variable(img_tensor.unsqueeze(0))\n",
    "logit = net(img_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.591 -> no flood\n",
      "0.409 -> flood/water\n",
      "output CAM.jpg for the top1 prediction: no flood\n",
      "output CAM.jpg for the top1 prediction: flood/water\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the imagenet category list\n",
    "classes = {0:\"no flood\", 1:\"flood/water\"}\n",
    "\n",
    "h_x = F.softmax(logit, dim=1).data.squeeze()\n",
    "probs, idx = h_x.sort(0, True)\n",
    "probs = probs.numpy()\n",
    "idx = idx.numpy()\n",
    "\n",
    "# output the prediction\n",
    "for i in range(2):\n",
    "    print('{:.3f} -> {}'.format(probs[i], classes[idx[i]]))\n",
    "\n",
    "# generate class activation mapping for the top1 prediction\n",
    "CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0]])\n",
    "\n",
    "# render the CAM and output\n",
    "print('output CAM.jpg for the top1 prediction: %s'%classes[idx[0]])\n",
    "img = cv2.imread('test_image.jpg')\n",
    "height, width, _ = img.shape\n",
    "heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
    "result = heatmap * 0.3 + img * 0.5\n",
    "cv2.imwrite('CAM_noflood.jpg', result)\n",
    "\n",
    "# generate class activation mapping for the flooding\n",
    "CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[1]])\n",
    "\n",
    "# render the CAM and output\n",
    "print('output CAM.jpg for the top1 prediction: %s'%classes[idx[1]])\n",
    "img = cv2.imread('test_image.jpg')\n",
    "height, width, _ = img.shape\n",
    "heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
    "result = heatmap * 0.3 + img * 0.5\n",
    "cv2.imwrite('CAM_flood.jpg', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this work? It turns out that, if you were to open up a CNN and examine the weights of the different layers, you would find that they tend to record key features of identifiers of specific objects:\n",
    "\n",
    "<img src=\"notebook_images/cnn.png\" width=\"750\"  />\n",
    "\n",
    "If your neural network has a specific architecture, you can essentially reconstruct a heatmap of where it is that those features are \"activated\" in an image. The architecture that Zhou et al used took advantage of a final global average pooling layer, which is also present in our ResNet architecture. This is why we chose to make our network following that architecture.\n",
    "\n",
    "Think about how amazing that is! We never once provided the network the ability to localize objects in an image. This is something that the network learns entirely on its own. \n",
    "\n",
    "### Exercise\n",
    "Look for a set of 2-3 images that you believe are emblematic of the class that you are aiming for. Apply the Class Activation Mapping procedure to them. Is the neural network able to localize key features of the class? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
